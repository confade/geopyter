{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "geopyter": {
     "Contributors": [
      "Michele Ferretti (https://github.com/miccferr)",
      "Jon Reades (https://github.com/jreades)",
      "James Millington (https://github.com/jamesdamillington)",
      "Jon Reades (https://github.com/jreades)"
     ],
     "git": {
      "active_branch": "master",
      "author.name": "Jon Reades",
      "authored_date": "2017-06-05 16:12:59",
      "committed_date": "2017-06-05 16:12:59",
      "committer.name": "Jon Reades",
      "sha": "2fce1fab4c3c28acabee493a24302e5ee1e8a1eb"
     }
    }
   },
   "source": [
    "# Delayed Gratification\n",
    "\n",
    "- Contributors: Jon Reades (jon@reades.com); James Millington (jamesdamillington@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming is about Delayed Gratification\n",
    "\n",
    "Learning to program involves a _lot_ of delayed gratification: you will need to invest quite a bit of time in learning to crawl before you can walk, and even more time in learning to walk before you can run. A well-designed programming language like Python can make it a little bit _easier_ to learn each step, but it still _**won't make it easy**_. \n",
    "\n",
    "We've said this before and we'll say it again (and again, and again) that you *must* think of this as learning a language: at first very little will make sense and you won't be able to say much more than 'hello, my name is X', but if you *practice* (**regularly**, cramming doesn't work in the same way that it doesn't work for learning French or Chienese) and really *think* about what you're doing it will get easier and easier, and faster and faster, to program.\n",
    "\n",
    "Learning to download files from the Internet using *only* code is a good case in point: why learn to do this when you could just open up a web browser, navigate to the site with the file you want, and then save it to your Downloads folder?\n",
    "\n",
    "Several reasons:\n",
    "\n",
    "1. Because this is annoying and time-consuming if you have to download *lots* of files (e.g. satellite imagery or Census data), or if you need to update your results regularly (e.g. weather or tweets). If you do this using code then you can get the computer to do the work for you! We can write code to talk to a web server, ask for a list of files (or scrape a list of files from a web page), and then download each one in turn... all from you clicking 'Run'.\n",
    "\n",
    "2. Because you can also organise your data a lot more logically. By default, when you download a file using a web browser it will save it to your Downloads folder with a name that may or may not make much sense to you. Using code, not only can we specify exactly where we want the data saved (and this can include doing things like creating new directories automatically for each month or Census variable), but we can even *automate* the naming of the file so that it is something easy for us to read.\n",
    "\n",
    "3. A not so obvious benefit is that you will need to think _logically_ and in an _organised_ way about how you manage data when you start doing data analysis, and using code to do all of this forces you to be logical right from the start: you will almost never receive raw data that doesn't require some 'pre-processing' work; so if you save your raw data and your processed data in the same directory how can you be sure you've loaded the right file? It's much easier to have two folders -- `raw` or `source`, and `clean` or `output` -- and use Python to read in raw data from one directory and then write the processed data out to a different directory than it would be using Excel's `File > Save As`. That's because the path is just _text_ and you can easily work with it that way!\n",
    "\n",
    "4. Also, these solutions are *scalable*: once you've learned how to write the code to scrape data from one web site, you can use the _same_ code to scrape data from a *different* computer somewhere else on the Internet. So instead of solving *one* specific problem, you are actually solving a whole *class* of problems at once! That's being lazy in a good way.\n",
    "\n",
    "Solving problems. Thinking logically. What employer wouldn't love someone with those skills?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Chaining\n",
    "\n",
    "Here's an example of this 'in action'. In and of itself the advantages of the Terminal (a.k.a. shell) might not be obvious, but we can write code in the Terminal that _chains_ commands together to do several things in one go. **You do not *need* to run this example** because it's for illustrative purposes:\n",
    "\n",
    "```\n",
    "curl -L http://bit.ly/2vrUFKi | head -3 | awk -F\",\" '{ print $2, $4, $6; }'\n",
    "```\n",
    "\n",
    "**_Note_**: If you _do_ want to run the code above then you will probably need to install 'Unix utilities' on your computer:\n",
    "- **On a Mac** you will need to run (from the Terminal!) the following command: `xcode-select --install`. This installs the `curl` utility along with a bunch of other useful applications.\n",
    "- **On Windows** you will probably need to run something like `conda install posix` but unless you want to donate a Windows machine I can't test this.\n",
    "- Once that's done you should copy+paste the above on to _one line_ of the Terminal, hit 'Return' and see some output from [this file](http://bit.ly/2vrUFKi) printed to the Terminal.\n",
    "\n",
    "This command might seem _really_ cryptic but we can break this problem down into steps (as I did when trying to remember how to do it!):\n",
    "\n",
    "1. `curl` -- this is a tool that allows you to download a file from the internet using only the Terminal, and `-L` is a 'flag' that tells curl to follow any redirects issued by the server.\n",
    "2. `head` -- this utility works with the _top_ part of a file (`tail` starts working with the _end_ of a file) and `-3` is also a flag which means 'take only the _first_ line of the file (so `head -10` would take the first _ten_ lines of the file).\n",
    "3. We 'glue' the output of `curl` together with `head` using `|` (known as a 'pipe') -- this tells the computer to pass the output of `curl` to the input of `head` (so head sees this as a file).\n",
    "4. We then direct the output of `head` to a *third* command which splits the output of head on commas (`-f\",\"`) and then outputs the 2nd, 4th, and 6th columns of the file.\n",
    "\n",
    "So this one line of _code_ allows you to something quite complex (normally involving a web browser, Microsoft Excel, and some selecting and copying) all in one short command. The point of this is that we are gaining in power at the cost of 'ease of use'. All of this *might* be a bit faster to do 'by hand' if you only need to do it *once*, but when we start doing analysis you almost never need to do something only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example that does basically the _same_ thing (and then some) as the Terminal code above, but using Python code (and a larger file) instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests as r\n",
    "\n",
    "url = 'http://bit.ly/2iIK9bA'\n",
    "data = r.get(url)\n",
    "content = data.content.decode('utf-8').splitlines()\n",
    "cr = csv.reader(content)\n",
    "\n",
    "for row in cr:\n",
    "    if row[3] != 'Population':\n",
    "        print(row[1] + \" has a population of \" + \"{:,}\".format(int(row[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to highlight what we've just done:\n",
    "\n",
    "1. We retrieved a CSV file from somewhere on this planet using `requests`,\n",
    "2. We 'parsed' it so that each `row` became something from which we could extract variables thanks to the magic of the `csv` library that we `import`,\n",
    "3. We skip the first row because it's a 'header' row and doesn't contain data,\n",
    "4. We print out the 2nd and 4th columns of the CSV file.\n",
    "\n",
    "You can see what the source looked like [here](http://bit.ly/2iIK9bA). And you'll note that the URL looks _exactly_ like the path that we saw when we used the Terminal to navigate between directories. \n",
    "\n",
    "So if you grasp the concept in one context, you can apply it in others. _That_ is scalability!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "geopyter": {
   "Contributors": [
    "Michele Ferretti (https://github.com/miccferr)",
    "Jon Reades (https://github.com/jreades)",
    "James Millington (https://github.com/jamesdamillington)"
   ],
   "git": {
    "active_branch": "master",
    "author.name": "Jon Reades",
    "authored_date": "2017-06-05 16:12:59",
    "committed_date": "2017-06-05 16:12:59",
    "committer.name": "Jon Reades",
    "sha": "2fce1fab4c3c28acabee493a24302e5ee1e8a1eb"
   },
   "libs": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
